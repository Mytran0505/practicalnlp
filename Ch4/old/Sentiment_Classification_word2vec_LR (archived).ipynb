{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/linanqiu/word2vec-sentiments/blob/master/word2vec-sentiment.ipynb\n",
    "    \n",
    "https://github.com/bgshin/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports \n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/root1/anuj_work/Github_repos/NLP_by_Example/Ch9'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "# load Google's word2vec model\n",
    "\n",
    "#path_to_model = '../../jupyters/vocab_coverage/word2vec_web/GoogleNews-vectors-negative300.bin'\n",
    "path_to_model = './../../../GoogleNews-vectors-negative300.bin'\n",
    "#w2v_model = Word2Vec.load_word2vec_format(path_to_model, binary=True)\n",
    "w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "print('done loading Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# get word2vec's vocab\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "\n",
    "print len(word2vec_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tag(datasets=None):\n",
    "    ''' Imports the datasets into one of two dictionaries\n",
    "\n",
    "        Dicts:\n",
    "            train & test\n",
    "\n",
    "        Keys:\n",
    "            values >= 4  are \"Positive\" in both Dictionaries\n",
    "\n",
    "        '''\n",
    "    if datasets is not None:\n",
    "        train_dict = {}\n",
    "        test_dict = {}\n",
    "        \n",
    "        #train_X = []\n",
    "        #train_Y = [] \n",
    "        train_Y = [] \n",
    "        test_Y = []\n",
    "        \n",
    "        print datasets.items()\n",
    "\n",
    "        for k, v in datasets.items(): #gives a list of tuples to iterate\n",
    "            with open(k) as fpath:\n",
    "                data = fpath.readlines()\n",
    "                \n",
    "                print len(data)\n",
    "                \n",
    "            for val, each_line in enumerate(data):\n",
    "                \n",
    "                if v.endswith(\"NEG\") and v.startswith(\"TRAIN\"):\n",
    "                    try:\n",
    "                        train_dict[val] = each_line\n",
    "                        #train_Y[val] = 0\n",
    "                        train_Y.append(0)\n",
    "                    except IndexError:\n",
    "                        print val\n",
    "                elif v.endswith(\"POS\") and v.startswith(\"TRAIN\"):\n",
    "                    train_dict[val + 12500] = each_line\n",
    "                    train_Y.append(1)\n",
    "                \n",
    "                elif v.endswith(\"NEG\") and v.startswith(\"TEST\"):\n",
    "                    try:\n",
    "                        test_dict[val] = each_line\n",
    "                        #test_Y[val] = 0\n",
    "                        test_Y.append(0)\n",
    "                    except IndexError:\n",
    "                        print val\n",
    "                else:\n",
    "                    test_dict[val + 12500] = each_line\n",
    "                    #test_Y[val] = 1\n",
    "                    test_Y.append(1)\n",
    "                        \n",
    "                        \n",
    "        return train_dict, test_dict,train_Y, test_Y\n",
    "    else:\n",
    "        print('Data not found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd ./../../../dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd /home/root1/anuj_work/Github_repos/NLP_by_Example/Ch10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_locations = {'./../../../dataset/test-neg.txt': 'TEST_NEG',\n",
    "                  './../../../dataset/test-pos.txt': 'TEST_POS',\n",
    "                  './../../../dataset/train-neg.txt': 'TRAIN_NEG',\n",
    "                  './../../../dataset/train-pos.txt': 'TRAIN_POS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s ./../../../dataset/test-neg.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_locations = {'./../../jupyters/Data/test-neg.txt': 'TEST_NEG',\n",
    "                  './../../jupyters/Data/test-pos.txt': 'TEST_POS',\n",
    "                  './../../jupyters/Data/train-neg.txt': 'TRAIN_NEG',\n",
    "                  './../../jupyters/Data/train-pos.txt': 'TRAIN_POS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./../../../dataset/test-neg.txt', 'TEST_NEG'), ('./../../../dataset/train-neg.txt', 'TRAIN_NEG'), ('./../../../dataset/test-pos.txt', 'TEST_POS'), ('./../../../dataset/train-pos.txt', 'TRAIN_POS')]\n",
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_dict, test_dict, train_Y,test_Y = import_tag(data_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of it s singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level it s better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [document.lower().replace('\\n', '').split() for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Tokenising...\n"
     ]
    }
   ],
   "source": [
    "# combine training and test reviews (50K) and tokenize each one of them\n",
    "\n",
    "print('Loading Data...')\n",
    "combined = train_dict.values() + test_dict.values()\n",
    "print('Tokenising...')\n",
    "combined = tokenizer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> 50000\n",
      "['story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', 'a', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'it', 's', 'singers', 'unfortunately', 'it', 'stays', 'absurd', 'the', 'whole', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', 'the', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'to', 'a', 'third', 'grader', 'on', 'a', 'technical', 'level', 'it', 's', 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'stars', 'sally', 'kirkland', 'and', 'frederic', 'forrest', 'can', 'be', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "print type(combined), len(combined)\n",
    "print combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get the vocab size of w2v_model\n",
    "len(w2v_model.vocab.keys())\n",
    "\n",
    "# this is one way to create vocab set from our dataset \n",
    "flat_list = chain(*combined)\n",
    "dataset_vocab = set(flat_list)\n",
    "len(dataset_vocab)\n",
    "dataset_vocab_lower = [key.lower() for key in dataset_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100673\n",
      "41929\n"
     ]
    }
   ],
   "source": [
    "# This does 2 things - \n",
    "# 1) Create a small dictionary of word to vectors for words which are only present in our dataset\n",
    "# 2) Count the number of words in our corpus not present in Google's word to vec\n",
    "\n",
    "DIMENSION = 300\n",
    "zero_vector = np.zeros(DIMENSION)\n",
    "\n",
    "our_dict = {}\n",
    "missing_count = 0\n",
    "\n",
    "for key in dataset_vocab_lower:\n",
    "    if key in w2v_model:\n",
    "        our_dict[key] = w2v_model[key]\n",
    "    else:\n",
    "        missing_count += 1\n",
    "        #print(\"missing key %s\" %(key)) \n",
    "        our_dict[key] = zero_vector\n",
    "\n",
    "print len(dataset_vocab_lower)\n",
    "print missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch word vector for each word. Add all the word vectors in a column sum fashion to get a \n",
    "# single vector of (DIMENSION,)\n",
    "\n",
    "def getWordVecs(words, w2v_dict):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            vecs.append(w2v_dict[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    vecs = np.array(vecs, dtype='float')\n",
    "    final_vec = np.sum(vecs, axis=0)\n",
    "    return final_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(combined, w2v_dict, train_X, train_Y, test_X, test_Y):\n",
    "    \n",
    "    for i, data in enumerate(combined):\n",
    "        try:\n",
    "            vector_rep = getWordVecs(data, w2v_dict)\n",
    "        except IndexError:\n",
    "            print i\n",
    "        \n",
    "        if i < 25000:\n",
    "            train_X.append(vector_rep)\n",
    "        else:\n",
    "            test_X.append(vector_rep)\n",
    "            \n",
    "        train_X_ = np.array(train_X, dtype='float')\n",
    "        train_X_.shape\n",
    "\n",
    "        train_Y_ = np.array(train_Y, dtype='int')\n",
    "        train_Y_.shape\n",
    "\n",
    "        test_X_ = np.array(test_X, dtype='float')\n",
    "        test_X_.shape\n",
    "\n",
    "        test_Y_ = np.array(test_Y, dtype='int')\n",
    "        test_Y_.shape\n",
    "\n",
    "    \n",
    "    return train_X_, train_Y_, test_X_, test_Y_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "test_X = []\n",
    "train_X, train_Y, test_X, test_Y = process_data(combined, our_dict, train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print type(train_X)\n",
    "print type(test_X)\n",
    "\n",
    "print type(train_Y)\n",
    "print type(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have the data loaded, processed and featureized (using word vectors)\n",
    "\n",
    "# we are now ready top train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize if the features are good enough to give a clean seperation\n",
    "\n",
    "# we do this using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing done, lets run a classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. With little efforts we got 85% accuracy. Thats a great basline to have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
